#######################################################################################################################
# 1. VERÄ° SETÄ° HÄ°KAYESÄ° VE PROBLEM TANIMI
#######################################################################################################################
print("\n--- VERÄ° SETÄ° HÄ°KAYESÄ° VE PROBLEM TANIMI ---\n")

print("""
Proje: ğŸ¦ Banka DolandÄ±rÄ±cÄ±lÄ±k Tespiti (Fraud Detection) 

Veri Seti Hikayesi: 
10 milyondan fazla mÃ¼ÅŸterisiyle faaliyet gÃ¶steren bÃ¼yÃ¼k bir dijital bankanÄ±n son yÄ±llarda artan
Ã§evrimiÃ§i iÅŸlem hacmiyle birlikte dolandÄ±rÄ±cÄ±lÄ±k vakalarÄ±nda da ciddi bir artÄ±ÅŸ yaÅŸanmÄ±ÅŸtÄ±r. Banka, bu durumu Ã¶nlemek
ve mÃ¼ÅŸterilerini korumak adÄ±na kapsamlÄ± bir makine Ã¶ÄŸrenmesi tabanlÄ± dolandÄ±rÄ±cÄ±lÄ±k tespit sistemi geliÅŸtirmek Ã¼zere
veri bilimi ekibini gÃ¶revlendirmiÅŸtir.

Problem TanÄ±mÄ±: 
BankanÄ±n 6 aylÄ±k iÅŸlem geÃ§miÅŸini iÃ§eren anonimleÅŸtirilmiÅŸ bir veri seti oluÅŸturulmuÅŸtur. Veri seti hem bireysel
hem de ticari mÃ¼ÅŸterilere ait iÅŸlemleri kapsar. AmaÃ§, bu iÅŸlemlerin hangilerinin dolandÄ±rÄ±cÄ±lÄ±k iÃ§erdiÄŸini tahmin eden
bir model geliÅŸtirmektir.
""")

# DeÄŸiÅŸken TanÄ±mlarÄ±
"""
fraud_bool
DolandÄ±rÄ±cÄ±lÄ±k etiketi (1 dolandÄ±rÄ±cÄ±lÄ±k, 0 normal iÅŸlem)

income
BaÅŸvuranÄ±n yÄ±llÄ±k geliri, kantillerle ifade edilmiÅŸtir. [0, 1] aralÄ±ÄŸÄ±ndadÄ±r.

name_email_similarity
E-posta ile baÅŸvuranÄ±n adÄ± arasÄ±ndaki benzerlik Ã¶lÃ§Ã¼tÃ¼. YÃ¼ksek deÄŸerler daha yÃ¼ksek benzerliÄŸi gÃ¶sterir. [0, 1] aralÄ±ÄŸÄ±ndadÄ±r.

prev_address_months_count
BaÅŸvuranÄ±n Ã¶nceki kayÄ±tlÄ± adresinde geÃ§irdiÄŸi ay sayÄ±sÄ±, yani varsa Ã¶nceki ikamet sÃ¼resi. [-1, 380] ay aralÄ±ÄŸÄ±nda (-1 eksik deÄŸer).

current_address_months_count
BaÅŸvuranÄ±n ÅŸu anki kayÄ±tlÄ± adresinde geÃ§irdiÄŸi ay sayÄ±sÄ±. [-1, 406] ay aralÄ±ÄŸÄ±nda (-1 eksik deÄŸer).

customer_age
BaÅŸvuranÄ±n yaÅŸÄ±, on yÄ±llÄ±k aralÄ±klarda gruplanmÄ±ÅŸ (Ã¶rneÄŸin, 20-29 yaÅŸ aralÄ±ÄŸÄ± 20 olarak temsil edilir).

days_since_request
BaÅŸvurunun yapÄ±lmasÄ±ndan bu yana geÃ§en gÃ¼n sayÄ±sÄ±. [0, 78] gÃ¼n aralÄ±ÄŸÄ±nda.

intended_balcon_amount
BaÅŸvuru iÃ§in baÅŸlangÄ±Ã§ta aktarÄ±lan tutar. [-1, 108] aralÄ±ÄŸÄ±nda.

payment_type
Kredi Ã¶deme planÄ± tÃ¼rÃ¼. 5 farklÄ± (anonimleÅŸtirilmiÅŸ) deÄŸer olabilir.

zip_count_4w
Son 4 haftada aynÄ± posta kodunda yapÄ±lan baÅŸvuru sayÄ±sÄ±. [1, 5767] aralÄ±ÄŸÄ±nda.

velocity_6h
Son 6 saatte yapÄ±lan toplam baÅŸvurularÄ±n hÄ±zÄ±, yani saat baÅŸÄ±na ortalama baÅŸvuru sayÄ±sÄ±. [-211, 24763] aralÄ±ÄŸÄ±nda.

velocity_24h
Son 24 saatte yapÄ±lan toplam baÅŸvurularÄ±n hÄ±zÄ±, yani saat baÅŸÄ±na ortalama baÅŸvuru sayÄ±sÄ±. [1329, 9527] aralÄ±ÄŸÄ±nda.

velocity_4w
Son 4 haftada yapÄ±lan toplam baÅŸvurularÄ±n hÄ±zÄ±, yani saat baÅŸÄ±na ortalama baÅŸvuru sayÄ±sÄ±. [2779, 7043] aralÄ±ÄŸÄ±nda.

bank_branch_count_8w
SeÃ§ilen banka ÅŸubesinde son 8 haftada yapÄ±lan toplam baÅŸvuru sayÄ±sÄ±. [0, 2521] aralÄ±ÄŸÄ±nda.

date_of_birth_distinct_emails_4w
Son 4 haftada aynÄ± doÄŸum tarihine sahip baÅŸvuranlar iÃ§in kullanÄ±lan e-posta sayÄ±sÄ±. [0, 42] aralÄ±ÄŸÄ±nda.

employment_status
BaÅŸvuranÄ±n istihdam durumu. 7 farklÄ± (anonimleÅŸtirilmiÅŸ) deÄŸer olabilir.

credit_risk_score
BaÅŸvurunun riskine iliÅŸkin iÃ§ deÄŸerlendirme puanÄ±. [-176, 387] aralÄ±ÄŸÄ±nda.

email_is_free
BaÅŸvuru e-postasÄ±nÄ±n alan adÄ± (Ã¼cretsiz ya da Ã¼cretli).

housing_status
BaÅŸvuranÄ±n mevcut konut durumu. 7 farklÄ± (anonimleÅŸtirilmiÅŸ) deÄŸer olabilir.

phone_home_valid
Verilen ev telefonu numarasÄ±nÄ±n geÃ§erliliÄŸi.

phone_mobile_valid
Verilen cep telefonu numarasÄ±nÄ±n geÃ§erliliÄŸi.

bank_months_count
Ã–nceki hesabÄ±n (varsa) kaÃ§ aydÄ±r aÃ§Ä±k olduÄŸu. [-1, 31] ay aralÄ±ÄŸÄ±nda (-1 eksik deÄŸer).

has_other_cards
BaÅŸvuranÄ±n aynÄ± bankadan baÅŸka kartlara sahip olup olmadÄ±ÄŸÄ±.

proposed_credit_limit
BaÅŸvuranÄ±n Ã¶nerdiÄŸi kredi limiti. [200, 2000] aralÄ±ÄŸÄ±nda.

foreign_request
BaÅŸvurunun yapÄ±ldÄ±ÄŸÄ± Ã¼lke ile bankanÄ±n bulunduÄŸu Ã¼lke farklÄ±ysa.

source
BaÅŸvurunun yapÄ±ldÄ±ÄŸÄ± Ã§evrim iÃ§i kaynak. TarayÄ±cÄ± (INTERNET) veya mobil uygulama (APP).

session_length_in_minutes
Banka sitesinde kullanÄ±cÄ±nÄ±n oturum sÃ¼resi (dakika cinsinden). [-1, 107] dakika aralÄ±ÄŸÄ±nda.

device_os
BaÅŸvurunun yapÄ±ldÄ±ÄŸÄ± cihazÄ±n iÅŸletim sistemi. OlasÄ± deÄŸerler: Windows, Macintox, Linux, X11 ya da diÄŸer.

keep_alive_session
Oturum kapatma tercihi (kullanÄ±cÄ± ayarÄ±).

device_distinct_emails_8w
AynÄ± cihazdan son 8 haftada bankacÄ±lÄ±k sitesine eriÅŸim saÄŸlayan farklÄ± e-posta adresi sayÄ±sÄ±. [0, 3] aralÄ±ÄŸÄ±nda.

device_fraud_count
AynÄ± cihazla yapÄ±lan dolandÄ±rÄ±cÄ±lÄ±k iÃ§erikli baÅŸvuru sayÄ±sÄ±. [0, 1] aralÄ±ÄŸÄ±nda.

month
BaÅŸvurunun yapÄ±ldÄ±ÄŸÄ± ay. [0, 7] aralÄ±ÄŸÄ±nda
"""

# Paketleri yÃ¼klemek iÃ§in requirements.txt dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
# !pip install -r requirements.txt

# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.cluster import KMeans
from imblearn.over_sampling import SMOTE
from collections import Counter
from scipy import stats
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings("ignore")

# GÃ¶rselleÅŸtirme ayarlarÄ±
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.width', 500)



#######################################################################################################################
# 2. KEÅÄ°FÃ‡Ä° VERÄ° ANALÄ°ZÄ° (EDA)
#######################################################################################################################
print("\n--- KEÅÄ°FÃ‡Ä° VERÄ° ANALÄ°ZÄ° ---\n")

# Veri seti yÃ¼kleme ve okutma iÅŸlemi
data = pd.read_csv("datasets/Base.csv")

# Veri setine genel bakÄ±ÅŸ ve hÄ±zlÄ± Ã¶nizleme
def check_df(dataframe, head=5):
    print("##################### Shape #####################")
    print(dataframe.shape)
    print("##################### Types #####################")
    print(dataframe.dtypes)
    print("##################### Head #####################")
    print(dataframe.head(head))
    print("##################### Tail #####################")
    print(dataframe.tail(head))
    print("##################### NA #####################")
    print(dataframe.isnull().sum())
    print("##################### Descriptives #########")
    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

check_df(data)

# Hedef deÄŸiÅŸken daÄŸÄ±lÄ±mÄ± grafiÄŸi
print(data["fraud_bool"].value_counts())
sns.countplot(x="fraud_bool", data=data)
plt.title("Fraud DaÄŸÄ±lÄ±mÄ±")
plt.show()

# Ã–rneklem al
if len(data) > 50000:
    data = data.sample(n=50000, random_state=42)

data["customer_id"] = data.index

# SayÄ±sal ve kategorik sÃ¼tunlarÄ± ayÄ±r
numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = data.select_dtypes(include='object').columns

# Eksik deÄŸer kontrolÃ¼
print("\nEksik DeÄŸerler:\n", data.isnull().sum())



#######################################################################################################################
# 3. VERÄ° Ã–N Ä°ÅLEME ve Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°
#######################################################################################################################
print("\n--- VERÄ° Ã–N Ä°ÅLEME ve Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ° ---\n")

# SayÄ±sal sÃ¼tunlar iÃ§in median ile doldurma
imputer = SimpleImputer(strategy='median')
data_numeric_imputed = pd.DataFrame(imputer.fit_transform(data[numeric_cols]), columns=numeric_cols)

# Kategorik sÃ¼tunlarÄ± encode et
label_encoders = {}
data_categorical_encoded = pd.DataFrame()
for column in categorical_cols:
    le = LabelEncoder()
    data_categorical_encoded[column] = le.fit_transform(data[column].astype(str))
    label_encoders[column] = le

# SayÄ±sal ve kategorik verileri birleÅŸtir
data_imputed = pd.concat([data_numeric_imputed, data_categorical_encoded], axis=1)

# Ã–zellik MÃ¼hendisliÄŸi
# 1. Zaman tabanlÄ± Ã¶zellikler
data_imputed['days_since_request_log'] = np.log1p(data_imputed['days_since_request'])
data_imputed['address_ratio'] = data_imputed['current_address_months_count'] / (data_imputed['prev_address_months_count'] + 1)
data_imputed['address_total_time'] = data_imputed['current_address_months_count'] + data_imputed['prev_address_months_count']

# 2. HÄ±z ve oranlar
epsilon = 1e-10
data_imputed['velocity_ratio_6h_24h'] = data_imputed['velocity_6h'] / (data_imputed['velocity_24h'] + epsilon)
data_imputed['velocity_ratio_24h_4w'] = data_imputed['velocity_24h'] / (data_imputed['velocity_4w'] + epsilon)
data_imputed['velocity_ratio_6h_4w'] = data_imputed['velocity_6h'] / (data_imputed['velocity_4w'] + epsilon)

# 3. Risk skorlarÄ± ve oranlar
epsilon = 1e-10
data_imputed['risk_income_ratio'] = data_imputed['credit_risk_score'] / (data_imputed['income'] * 100 + epsilon)
data_imputed['risk_age_ratio'] = data_imputed['credit_risk_score'] / (data_imputed['customer_age'] + epsilon)

# 4. EtkileÅŸim Ã¶zellikleri
data_imputed['age_income_interaction'] = data_imputed['customer_age'] * data_imputed['income']
data_imputed['risk_session_interaction'] = data_imputed['credit_risk_score'] * data_imputed['session_length_in_minutes']
data_imputed['email_risk_interaction'] = data_imputed['name_email_similarity'] * data_imputed['credit_risk_score']

# 5. Kategorik deÄŸiÅŸkenlerden tÃ¼retilen Ã¶zellikler
data_imputed['payment_risk'] = data_imputed['payment_type'].astype(str) + '_' + data_imputed['credit_risk_score'].astype(str)
data_imputed['payment_risk'] = data_imputed['payment_risk'].astype('category').cat.codes

# 6. Basamak Ã¶zellikleri
data_imputed['risk_score_bin'] = pd.qcut(data_imputed['credit_risk_score'], q=5, labels=False, duplicates='drop')
data_imputed['velocity_24h_bin'] = pd.qcut(data_imputed['velocity_24h'], q=5, labels=False, duplicates='drop')
data_imputed['customer_age_bin'] = pd.cut(data_imputed['customer_age'], bins=[0, 25, 35, 50, 100], labels=False)

# 7. Boolean Ã¶zellikleri
data_imputed['is_high_risk'] = (data_imputed['credit_risk_score'] < 50).astype(int)
data_imputed['is_new_bank_customer'] = (data_imputed['bank_months_count'] < 3).astype(int)
data_imputed['is_high_velocity'] = (data_imputed['velocity_24h'] > data_imputed['velocity_24h'].median()).astype(int)

# 8. Anomali skorlarÄ±
numeric_features = data_imputed.select_dtypes(include=['float64', 'int64']).columns
for col in ['velocity_6h', 'velocity_24h', 'velocity_4w', 'credit_risk_score']:
    if col in data_imputed.columns:
        data_imputed[f'{col}_zscore'] = stats.zscore(data_imputed[col], nan_policy='omit')
        data_imputed[f'is_{col}_outlier'] = ((data_imputed[f'{col}_zscore'] > 3) | (data_imputed[f'{col}_zscore'] < -3)).astype(int)

# 9. Logaritmik ve kÃ¶k dÃ¶nÃ¼ÅŸÃ¼mleri
for col in ['income', 'credit_risk_score', 'velocity_24h', 'velocity_4w']:
    if col in data_imputed.columns:
        # Negatif deÄŸerler iÃ§in gÃ¼venli log dÃ¶nÃ¼ÅŸÃ¼mÃ¼
        data_imputed[f'{col}_log'] = np.log1p(np.maximum(0, data_imputed[col]))
        # Negatif deÄŸerler iÃ§in gÃ¼venli kÃ¶k dÃ¶nÃ¼ÅŸÃ¼mÃ¼
        data_imputed[f'{col}_sqrt'] = np.sqrt(np.maximum(0, data_imputed[col]))

# 10. Ã‡oklu deÄŸiÅŸken etkileÅŸimleri
data_imputed['risk_velocity_age'] = data_imputed['credit_risk_score'] * data_imputed['velocity_24h'] * data_imputed['customer_age']
data_imputed['risk_income_age'] = data_imputed['credit_risk_score'] * data_imputed['income'] * data_imputed['customer_age']

# 11. Oransal Ã¶zellikler
if 'bank_months_count' in data_imputed.columns and 'customer_age' in data_imputed.columns:
    # MÃ¼ÅŸterinin yaÅŸÄ±na gÃ¶re banka kullanÄ±m sÃ¼resi oranÄ±
    data_imputed['bank_usage_lifetime_ratio'] = data_imputed['bank_months_count'] / (data_imputed['customer_age'] * 12 + 1)

epsilon = 1e-10
data_imputed['risk_velocity_ratio'] = data_imputed['credit_risk_score'] / (data_imputed['velocity_24h'] + epsilon)
data_imputed['email_device_ratio'] = data_imputed['date_of_birth_distinct_emails_4w'] / (data_imputed['device_distinct_emails_8w'] + epsilon)
data_imputed['bank_activity_ratio'] = data_imputed['bank_months_count'] / (data_imputed['bank_branch_count_8w'] + epsilon)

# 12. RFM (Recency, Frequency, Monetary)
if 'days_since_request' in data_imputed.columns and 'velocity_4w' in data_imputed.columns and 'income' in data_imputed.columns:
    # Her bir bileÅŸeni ayrÄ± ayrÄ± hesapla ve eksik deÄŸerleri kontrol et
    r_score = pd.qcut(data_imputed['days_since_request'], 5, labels=False, duplicates='drop')
    f_score = pd.qcut(data_imputed['velocity_4w'], 5, labels=False, duplicates='drop')
    m_score = pd.qcut(data_imputed['income'], 5, labels=False, duplicates='drop')

    # Eksik deÄŸerleri 0 ile doldur
    r_score = r_score.fillna(0).astype(int)
    f_score = f_score.fillna(0).astype(int)
    m_score = m_score.fillna(0).astype(int)

    # RFM skorunu hesapla
    data_imputed['rfm_score'] = r_score + f_score + m_score

# 13. KÃ¼meleme tabanlÄ± Ã¶zellikler
if len(data_imputed) > 1000:
    kmeans_features = ['credit_risk_score', 'income', 'velocity_24h', 'customer_age']
    if all(col in data_imputed.columns for col in kmeans_features):
        kmeans_df = data_imputed[kmeans_features].copy()

        # Eksik deÄŸerleri doldurma
        for col in kmeans_features:
            if kmeans_df[col].isnull().sum() > 0:
                kmeans_df[col].fillna(kmeans_df[col].median(), inplace=True)

        # Sonsuz deÄŸer kontrolÃ¼
        kmeans_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        for col in kmeans_features:
            if kmeans_df[col].isnull().sum() > 0:
                kmeans_df[col].fillna(kmeans_df[col].median(), inplace=True)

        # Veri Ã¶lÃ§eklendirme
        scaler = StandardScaler()
        kmeans_df_scaled = scaler.fit_transform(kmeans_df)

        # KÃ¼meleme modeli
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        data_imputed['risk_cluster'] = kmeans.fit_predict(kmeans_df_scaled)

        # Her kÃ¼me iÃ§in ortalama fraud oranÄ±
        cluster_fraud_rates = data_imputed.groupby('risk_cluster')['fraud_bool'].mean()

        # KÃ¼me fraud oranlarÄ±nÄ± Ã¶zellik olarak ekleme
        for cluster, rate in cluster_fraud_rates.items():
            data_imputed.loc[data_imputed['risk_cluster'] == cluster, 'cluster_fraud_rate'] = rate

        # KÃ¼me merkezlerine olan uzaklÄ±klarÄ± Ã¶zellik olarak ekleme
        cluster_centers = kmeans.cluster_centers_
        for i in range(len(cluster_centers)):
            # UzaklÄ±k hesabÄ±
            data_imputed[f'distance_to_cluster_{i}'] = np.sqrt(((kmeans_df_scaled - cluster_centers[i])**2).sum(axis=1))



# 14. Zamansal Ã¶rÃ¼ntÃ¼ ve frekans tabanlÄ± Ã¶zellikler
# HÄ±z tabanlÄ±
if all(col in data_imputed.columns for col in ['velocity_6h', 'velocity_24h', 'velocity_4w']):
    # KÄ±sa vadeli hÄ±z deÄŸiÅŸimi (6 saat - 24 saat)
    data_imputed['velocity_change_short'] = data_imputed['velocity_6h'] - (data_imputed['velocity_24h'] / 4)

    # Uzun vadeli hÄ±z deÄŸiÅŸimi (24 saat - 4 hafta)
    data_imputed['velocity_change_long'] = data_imputed['velocity_24h'] - (data_imputed['velocity_4w'] / 28)

    # HÄ±z deÄŸiÅŸim oranÄ± (kÄ±sa vadeli / uzun vadeli)
    epsilon = 1e-10
    data_imputed['velocity_change_ratio'] = np.abs(data_imputed['velocity_change_short']) / (np.abs(data_imputed['velocity_change_long']) + epsilon)

    # Anormal hÄ±z deÄŸiÅŸimi (z-score tabanlÄ±)
    data_imputed['velocity_change_short_zscore'] = stats.zscore(data_imputed['velocity_change_short'], nan_policy='omit')
    data_imputed['is_abnormal_velocity_change'] = (np.abs(data_imputed['velocity_change_short_zscore']) > 2).astype(int)

# Frekans tabanlÄ± Ã¶zellikler
if 'bank_branch_count_8w' in data_imputed.columns and 'bank_months_count' in data_imputed.columns:
    epsilon = 1e-10
    # HaftalÄ±k ortalama ÅŸube ziyareti
    data_imputed['avg_branch_visits_per_week'] = data_imputed['bank_branch_count_8w'] / 8

    # AylÄ±k ÅŸube ziyaret yoÄŸunluÄŸu
    data_imputed['branch_visit_intensity'] = data_imputed['bank_branch_count_8w'] / (data_imputed['bank_months_count'] + epsilon)

    # Anormal ÅŸube ziyaret yoÄŸunluÄŸu
    data_imputed['branch_visit_intensity_zscore'] = stats.zscore(data_imputed['branch_visit_intensity'], nan_policy='omit')
    data_imputed['is_abnormal_branch_activity'] = (data_imputed['branch_visit_intensity_zscore'] > 2).astype(int)

# Aktivite yoÄŸunluÄŸu Ã¶zellikleri
if 'session_length_in_minutes' in data_imputed.columns and 'velocity_24h' in data_imputed.columns:
    epsilon = 1e-10
    # Oturum baÅŸÄ±na aktivite yoÄŸunluÄŸu
    data_imputed['activity_per_minute'] = data_imputed['velocity_24h'] / (data_imputed['session_length_in_minutes'] + epsilon)

    # Anormal aktivite yoÄŸunluÄŸu
    data_imputed['activity_per_minute_zscore'] = stats.zscore(data_imputed['activity_per_minute'], nan_policy='omit')
    data_imputed['is_abnormal_activity_rate'] = (data_imputed['activity_per_minute_zscore'] > 2).astype(int)

# Ã–zellik mÃ¼hendisliÄŸi grafikleri
#Orijinal ve tÃ¼retilmiÅŸ Ã¶zelliklerin daÄŸÄ±lÄ±mÄ±
original_features = ['credit_risk_score', 'velocity_24h', 'income', 'customer_age']
derived_features = ['risk_income_ratio', 'velocity_ratio_24h_4w', 'rfm_score', 'is_high_risk']

# Orijinal Ã¶zelliklerin grafikleri
plt.figure(figsize=(15, 10))
for i, feature in enumerate(original_features):
    if feature in data_imputed.columns:
        plt.subplot(2, 2, i+1)
        sns.histplot(data_imputed[feature], kde=True)
        plt.title(f"Orijinal Ã–zellik: {feature}")
        plt.tight_layout()
fig = plt.gcf()  # GeÃ§erli figÃ¼rÃ¼ al
plt.show()
plt.close()

# TÃ¼retilmiÅŸ Ã¶zelliklerin grafikleri
plt.figure(figsize=(15, 10))
for i, feature in enumerate(derived_features):
    if feature in data_imputed.columns:
        plt.subplot(2, 2, i+1)
        sns.histplot(data_imputed[feature], kde=True)
        plt.title(f"TÃ¼retilmiÅŸ Ã–zellik: {feature}")
        plt.tight_layout()
fig = plt.gcf()
plt.show()
plt.close()

# Ã–zellik kategorileri bazÄ±nda Ã¶zellik sayÄ±larÄ±
feature_categories = {
    'Zaman TabanlÄ±': ['days_since_request_log', 'address_ratio', 'address_total_time'],
    'HÄ±z ve Oranlar': ['velocity_ratio_6h_24h', 'velocity_ratio_24h_4w', 'velocity_ratio_6h_4w'],
    'Risk SkorlarÄ±': ['risk_income_ratio', 'risk_age_ratio'],
    'EtkileÅŸim': ['age_income_interaction', 'risk_session_interaction', 'email_risk_interaction'],
    'Kategorik TÃ¼retilmiÅŸ': ['payment_risk'],
    'Basamak': ['risk_score_bin', 'velocity_24h_bin', 'customer_age_bin'],
    'Boolean': ['is_high_risk', 'is_new_bank_customer', 'is_high_velocity'],
    'Anomali': [col for col in data_imputed.columns if 'zscore' in col or 'outlier' in col],
    'Logaritmik/KÃ¶k': [col for col in data_imputed.columns if '_log' in col or '_sqrt' in col],
    'Ã‡oklu DeÄŸiÅŸken EtkileÅŸimleri': ['risk_velocity_age', 'risk_income_age'],
    'Oransal': ['bank_usage_lifetime_ratio', 'risk_velocity_ratio', 'email_device_ratio', 'bank_activity_ratio'],
    'RFM': ['rfm_score'],
    'KÃ¼meleme': [col for col in data_imputed.columns if 'cluster' in col or 'distance_to_cluster' in col],
    'Zamansal Ã–rÃ¼ntÃ¼': [col for col in data_imputed.columns if 'velocity_change' in col or 'activity_per_minute' in col]
}

# Her kategorideki geÃ§erli Ã¶zellik sayÄ±sÄ±nÄ± hesapla
category_counts = {}
for category, features in feature_categories.items():
    valid_features = [f for f in features if f in data_imputed.columns]
    category_counts[category] = len(valid_features)

# Kategorilere gÃ¶re Ã¶zellik sayÄ±larÄ±nÄ± gÃ¶rselleÅŸtir
plt.figure(figsize=(14, 8))
categories = list(category_counts.keys())
counts = list(category_counts.values())
# SayÄ±ya gÃ¶re sÄ±rala
sorted_indices = np.argsort(counts)[::-1]
sorted_categories = [categories[i] for i in sorted_indices]
sorted_counts = [counts[i] for i in sorted_indices]

sns.barplot(x=sorted_counts, y=sorted_categories, palette="viridis")
plt.title("Ã–zellik Kategorilerine GÃ¶re Ã–zellik SayÄ±larÄ±")
plt.xlabel("Ã–zellik SayÄ±sÄ±")
plt.ylabel("Ã–zellik Kategorisi")
plt.tight_layout()
fig = plt.gcf()
plt.show()
plt.close()

# Ã–zellik kategorilerinden Ã¶rnekler
# Her kategoriden bir Ã¶rnek Ã¶zellik seÃ§ip gÃ¶rselleÅŸtir
category_examples = {}
for category, features in feature_categories.items():
    valid_features = [f for f in features if f in data_imputed.columns]
    if valid_features:
        category_examples[category] = valid_features[0]  # Her kategoriden ilk geÃ§erli Ã¶zelliÄŸi al

# Kategorilere gÃ¶re Ã¶rnek Ã¶zellikleri gÃ¶rselleÅŸtir
if category_examples:
    # KaÃ§ satÄ±r ve sÃ¼tun olacaÄŸÄ±nÄ± hesapla
    n_examples = len(category_examples)
    n_cols = 3
    n_rows = (n_examples + n_cols - 1) // n_cols  # YukarÄ± yuvarlama

    plt.figure(figsize=(18, n_rows * 4))
    for i, (category, feature) in enumerate(category_examples.items()):
        plt.subplot(n_rows, n_cols, i+1)

        # Ã–zellik tipine gÃ¶re uygun gÃ¶rselleÅŸtirme yap
        if data_imputed[feature].dtype in ['int64', 'float64']:
            if data_imputed[feature].nunique() <= 5:  # Kategorik gibi davranan sayÄ±sal deÄŸiÅŸken
                sns.countplot(x=data_imputed[feature])
                plt.xticks(rotation=45)
            else:  # SÃ¼rekli sayÄ±sal deÄŸiÅŸken
                sns.histplot(data_imputed[feature], kde=True)
        else:  # Kategorik deÄŸiÅŸken
            top_categories = data_imputed[feature].value_counts().head(10).index
            sns.countplot(y=data_imputed[feature][data_imputed[feature].isin(top_categories)])

        plt.title(f"{category}: {feature}")
        plt.tight_layout()
    fig = plt.gcf()
    plt.show()
    plt.close()

# Korelasyon matrisi
# Orijinal ve tÃ¼retilmiÅŸ Ã¶zelliklerden Ã¶nemli olanlarÄ± seÃ§
important_features = original_features + derived_features
# Veri setinde olan Ã¶zellikleri filtrele
valid_features = [f for f in important_features if f in data_imputed.columns]

if len(valid_features) > 0:
    # Korelasyon matrisini hesapla
    corr_matrix = data_imputed[valid_features].corr()

    # Korelasyon matrisini gÃ¶rselleÅŸtir
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", 
                vmin=-1, vmax=1, linewidths=0.5)
    plt.title("Ã–nemli Ã–zellikler ArasÄ±ndaki Korelasyon")
    plt.tight_layout()
    fig = plt.gcf()
    plt.show()
    plt.close()

# Anomali tespiti gÃ¶rselleÅŸtirmesi
# Z-score deÄŸerlerini gÃ¶rselleÅŸtir
zscore_columns = [col for col in data_imputed.columns if 'zscore' in col]
if zscore_columns:
    plt.figure(figsize=(15, 10))
    for i, col in enumerate(zscore_columns[:4]):  # En fazla 4 z-score sÃ¼tunu gÃ¶ster
        plt.subplot(2, 2, i+1)
        sns.boxplot(y=data_imputed[col])
        plt.title(f"Z-Score DaÄŸÄ±lÄ±mÄ±: {col}")
        plt.axhline(y=3, color='r', linestyle='--', label='EÅŸik (3)')
        plt.axhline(y=-3, color='r', linestyle='--')
        plt.legend()

    plt.tight_layout()
    fig = plt.gcf()
    plt.show()
    plt.close()

    # Anomali oranlarÄ±nÄ± gÃ¶rselleÅŸtir
    outlier_columns = [col for col in data_imputed.columns if 'outlier' in col]
    if outlier_columns:
        outlier_rates = {}
        for col in outlier_columns:
            outlier_rates[col] = data_imputed[col].mean() * 100  # YÃ¼zde olarak anomali oranÄ±

        plt.figure(figsize=(12, 6))
        sns.barplot(x=list(outlier_rates.values()), y=list(outlier_rates.keys()), palette="rocket")
        plt.title("Ã–zellik BazÄ±nda Anomali OranlarÄ± (%)")
        plt.xlabel("Anomali OranÄ± (%)")
        plt.tight_layout()
        fig = plt.gcf()
        plt.show()
        plt.close()

# Fraud ile en Ã§ok iliÅŸkili Ã¶zellikleri bul
fraud_correlations = {}
for col in data_imputed.columns:
    if col != 'fraud_bool' and data_imputed[col].dtype in ['int64', 'float64']:
        corr = data_imputed[col].corr(data_imputed['fraud_bool'])
        if not np.isnan(corr):
            fraud_correlations[col] = corr

# En yÃ¼ksek korelasyona sahip 10 Ã¶zelliÄŸi seÃ§
top_fraud_features = sorted(fraud_correlations.items(), key=lambda x: abs(x[1]), reverse=True)[:10]
top_fraud_feature_names = [item[0] for item in top_fraud_features]
top_fraud_correlation_values = [item[1] for item in top_fraud_features]

# Fraud ile en Ã§ok iliÅŸkili Ã¶zellikleri gÃ¶rselleÅŸtir
plt.figure(figsize=(12, 8))
colors = ['red' if x < 0 else 'green' for x in top_fraud_correlation_values]
sns.barplot(x=top_fraud_correlation_values, y=top_fraud_feature_names, palette=colors)
plt.title("Fraud ile En Ã‡ok Ä°liÅŸkili 10 Ã–zellik")
plt.xlabel("Korelasyon KatsayÄ±sÄ±")
plt.axvline(x=0, color='black', linestyle='-')
plt.tight_layout()
fig = plt.gcf()
plt.show()
plt.close()

# En yÃ¼ksek korelasyona sahip 3 Ã¶zelliÄŸi seÃ§
if len(top_fraud_features) >= 3:
    top_3_features = [item[0] for item in top_fraud_features[:3]]

    plt.figure(figsize=(15, 10))
    for i, feature in enumerate(top_3_features):
        plt.subplot(2, 2, i+1)
        sns.histplot(
            data=data_imputed,
            x=feature,
            hue='fraud_bool',
            multiple='stack',
            palette=['green', 'red'],
            kde=True
        )
        plt.title(f"{feature} DaÄŸÄ±lÄ±mÄ± (Fraud vs Non-Fraud)")
        plt.xlabel(feature)
        plt.ylabel("Frekans")
        plt.legend(['Normal', 'Fraud'])

    # Fraud oranÄ± vs Ã¶zellik deÄŸeri grafiÄŸi
    if len(top_3_features) > 0:
        plt.subplot(2, 2, 4)
        feature = top_3_features[0]  # En yÃ¼ksek korelasyonlu Ã¶zellik

        # Ã–zelliÄŸi 10 dilime bÃ¶l ve her dilim iÃ§in fraud oranÄ±nÄ± hesapla
        bins = pd.qcut(data_imputed[feature], 10, duplicates='drop')
        fraud_rate_by_bin = data_imputed.groupby(bins)['fraud_bool'].mean() * 100

        # Bin merkezlerini hesapla
        bin_centers = [(x.left + x.right) / 2 for x in fraud_rate_by_bin.index]

        # Fraud oranÄ± vs Ã¶zellik deÄŸeri grafiÄŸi
        plt.plot(bin_centers, fraud_rate_by_bin.values, 'o-', linewidth=2, markersize=8)
        plt.title(f"Fraud OranÄ± vs {feature}")
        plt.xlabel(f"{feature} (DilimlenmiÅŸ)")
        plt.ylabel("Fraud OranÄ± (%)")
        plt.grid(True, linestyle='--', alpha=0.7)

    plt.tight_layout()
    fig = plt.gcf()
    plt.show()
    plt.close()

# Ã–zellik mÃ¼hendisliÄŸi Ã¶ncesi ve sonrasÄ± karÅŸÄ±laÅŸtÄ±rma
# Orijinal veri seti ve mÃ¼hendislik sonrasÄ± veri seti boyutlarÄ±
original_shape = data.shape
engineered_shape = data_imputed.shape
shapes = [original_shape[1], engineered_shape[1]]
labels = ['Orijinal Veri Seti', 'MÃ¼hendislik SonrasÄ±']

plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=shapes, palette="viridis")
plt.title("Ã–zellik MÃ¼hendisliÄŸi Ã–ncesi ve SonrasÄ± Ã–zellik SayÄ±sÄ±")
plt.ylabel("Ã–zellik SayÄ±sÄ±")

for i, v in enumerate(shapes):
    plt.text(i, v + 5, str(v), ha='center')

plt.tight_layout()
fig = plt.gcf()
plt.show()
plt.close()

# Ã–zellik mÃ¼hendisliÄŸinin model performansÄ±na etkisi

# SimÃ¼le edilmiÅŸ performans deÄŸerleri
feature_engineering_steps = [
    'Orijinal Ã–zellikler',
    '+ Zaman TabanlÄ±',
    '+ HÄ±z ve Oranlar',
    '+ Risk SkorlarÄ±',
    '+ EtkileÅŸim',
    '+ Anomali Tespiti',
    '+ DiÄŸer Ã–zellikler'
]

# SimÃ¼le edilmiÅŸ AUC deÄŸerleri
simulated_auc = [0.75, 0.78, 0.82, 0.85, 0.87, 0.89, 0.91]

plt.figure(figsize=(12, 6))
sns.lineplot(x=feature_engineering_steps, y=simulated_auc, marker='o', linewidth=2, markersize=10)
plt.title("Ã–zellik MÃ¼hendisliÄŸi AdÄ±mlarÄ±nÄ±n Model PerformansÄ±na Etkisi (SimÃ¼lasyon)")
plt.ylabel("AUC Skoru (SimÃ¼le EdilmiÅŸ)")
plt.xlabel("Ã–zellik MÃ¼hendisliÄŸi AdÄ±mlarÄ±")
plt.ylim(0.7, 0.95)
plt.grid(True, linestyle='--', alpha=0.7)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
plt.close()

# Ã–zellik SeÃ§imi
# 1. YÃ¼ksek korelasyonlu Ã¶zellikleri kaldÄ±r
def remove_highly_correlated_features(df, threshold=0.95):
    # Korelasyon matrisi hesapla
    corr_matrix = df.corr().abs()

    # Ãœst Ã¼Ã§geni al (korelasyon matrisi simetriktir)
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # EÅŸik deÄŸerinden yÃ¼ksek korelasyona sahip sÃ¼tunlarÄ± bul
    to_drop = [column for column in upper.columns if column != "customer_id" and any(upper[column] > threshold)]

    print(f"YÃ¼ksek korelasyonlu {len(to_drop)} Ã¶zellik kaldÄ±rÄ±ldÄ±")
    # KaldÄ±rÄ±lan Ã¶zellikleri yazdÄ±r
    print("KaldÄ±rÄ±lan Ã¶zellikler:", to_drop)
    return df.drop(to_drop, axis=1)

# Hedef deÄŸiÅŸkeni ayÄ±r
X_all = data_imputed.drop("fraud_bool", axis=1)
y = data_imputed["fraud_bool"]

# YÃ¼ksek korelasyonlu Ã¶zellikleri kaldÄ±r
X_reduced = remove_highly_correlated_features(X_all, threshold=0.95)

# Sonsuz deÄŸerleri NaN ile deÄŸiÅŸtir
X_reduced.replace([np.inf, -np.inf], np.nan, inplace=True)

# NaN deÄŸerleri medyan ile doldur
for col in X_reduced.columns:
    if X_reduced[col].isnull().sum() > 0:
        X_reduced[col].fillna(X_reduced[col].median(), inplace=True)

# AÅŸÄ±rÄ± bÃ¼yÃ¼k deÄŸerleri kontrol et ve kÄ±rp
for col in X_reduced.columns:
    # 99.9 persentil Ã¼zerindeki deÄŸerleri kÄ±rp
    upper_limit = X_reduced[col].quantile(0.999)
    X_reduced[col] = np.minimum(X_reduced[col], upper_limit)
    # 0.1 persentil altÄ±ndaki deÄŸerleri kÄ±rp
    lower_limit = X_reduced[col].quantile(0.001)
    X_reduced[col] = np.maximum(X_reduced[col], lower_limit)

print(f"Veri temizleme Ã¶ncesi sÃ¼tun sayÄ±sÄ±: {X_all.shape[1]}")
print(f"Veri temizleme sonrasÄ± sÃ¼tun sayÄ±sÄ±: {X_reduced.shape[1]}")

# 2. Ã–zellik Ã¶nem sÄ±ralamasÄ± iÃ§in basit bir model eÄŸit
# Basit bir model oluÅŸtur
feature_selector = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)
selector = SelectFromModel(feature_selector, threshold="median")

# Modeli eÄŸit ve Ã¶nemli Ã¶zellikleri seÃ§
use_fallback = False

selector.fit(X_reduced, y)
X_selected = selector.transform(X_reduced)
selected_feature_indices = selector.get_support(indices=True)
selected_feature_names = X_reduced.columns[selected_feature_indices]

if len(selected_feature_names) < 10:
    # EÄŸer seÃ§ilen Ã¶zellik sayÄ±sÄ± 10'dan azsa, tÃ¼m Ã¶zellikleri kullan
    use_fallback = True

# SeÃ§ilen Ã¶zelliklerin isimlerini yazdÄ±rma
print("\nSeÃ§ilen Ã–zellikler:")
print(selected_feature_names.tolist())

print(f"\nÃ–zellik seÃ§imi sonrasÄ± {len(selected_feature_names)} Ã¶zellik kaldÄ±")
print("\nSeÃ§ilen Ã¶zelliklerden bazÄ±larÄ±:", selected_feature_names[:10].tolist())

# SeÃ§ilen Ã¶zellikleri kullanarak veri Ã§erÃ§evesini gÃ¼ncelleme
if use_fallback:
    X = X_selected
else:
    X = X_reduced.iloc[:, selected_feature_indices]

# SMOTE ile dengeleme
print("\nSMOTE Uygulanmadan Ã–nce:", Counter(y))
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
print("SMOTE UygulandÄ±ktan Sonra:", Counter(y_resampled))

# EÄŸitim ve test verisi ayÄ±r
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Ã–zellik Ã¶lÃ§ekleme (StandardScaler)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)



#######################################################################################################################
# 4. MODELLEME
#######################################################################################################################
print("\n--- MODELLEME ---\n")

models = {
    "Logistic Regression": LogisticRegression(max_iter=3000, solver='saga', n_jobs=-1, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5, n_jobs=-1),
    "Decision Tree": DecisionTreeClassifier(max_depth=3, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=20, max_depth=3, random_state=42, n_jobs=-1),
    "XGBoost": XGBClassifier(n_estimators=20, max_depth=3, random_state=42, verbosity=0, n_jobs=-1),
    "LightGBM": LGBMClassifier(n_estimators=20, max_depth=3, random_state=42, n_jobs=-1)
}

# Model performanslarÄ±nÄ± sakla
results = []

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    train_roc_auc = roc_auc_score(y_train, model.predict_proba(X_train_scaled)[:, 1])
    test_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])
    report = classification_report(y_test, y_test_pred, output_dict=True)

    results.append({
        "Model": name,
        "Train Accuracy": train_acc,
        "Test Accuracy": test_acc,
        "Train ROC AUC": train_roc_auc,
        "Test ROC AUC": test_roc_auc,
        "Precision": report["weighted avg"]["precision"],
        "Recall": report["weighted avg"]["recall"],
        "F1-score": report["weighted avg"]["f1-score"]
    })

    print(f"{name}:")
    print(f"Train Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}")
    print(f"Train ROC AUC: {train_roc_auc:.4f} | Test ROC AUC: {test_roc_auc:.4f}")

# SonuÃ§larÄ± DataFrame olarak yazdÄ±r
results_df = pd.DataFrame(results)
print("\nModel KarÅŸÄ±laÅŸtÄ±rmalarÄ±:")
print(results_df.sort_values(by="Test ROC AUC", ascending=False))

# Overfitting kontrolÃ¼ iÃ§in eÄŸitim ve test skorlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±r
#
print("\nOverfitting KontrolÃ¼ (Train vs Test ROC AUC):")
for i, row in results_df.iterrows():
    diff = row["Train ROC AUC"] - row["Test ROC AUC"]
    print(f"{row['Model']}: Train-Test ROC AUC FarkÄ± = {diff:.4f}")

# En iyi 2 model iÃ§in confusion matrix ve classification report
best_models = results_df.sort_values(by="Test ROC AUC", ascending=False).head(2)["Model"].values

for name in best_models:
    model = models[name]
    y_pred = model.predict(X_test_scaled)
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
    plt.title(f"Confusion Matrix: {name}")
    plt.xlabel("Tahmin edilen")
    plt.ylabel("GerÃ§ek")
    plt.tight_layout()
    plt.show()
    plt.close()

    print(f"\n{name} Classification Report:")
    print(classification_report(y_test, y_pred))

# Random Forest iÃ§in feature importance (ilk 15 Ã¶zellik)
rf = models["Random Forest"]
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1][:15]
features = X.columns[indices]

# Ã–zellik Ã¶nem sÄ±ralamasÄ±nÄ± gÃ¶rselleÅŸtir
plt.figure(figsize=(12, 8))
sns.barplot(x=importances[indices], y=features, palette="viridis")
plt.title("Random Forest - En Ã–nemli 15 Ã–zellik")
plt.xlabel("Ã–nem Derecesi")
plt.tight_layout()
plt.show()
plt.close()

# MÃ¼hendislik yapÄ±lmÄ±ÅŸ Ã¶zelliklerin Ã¶nem analizi
engineered_features = [col for col in X.columns if col not in data.columns]
engineered_indices = [i for i, col in enumerate(X.columns) if col in engineered_features]

if engineered_indices and len(importances) > 0:
    # Sadece geÃ§erli indeksleri al
    valid_indices = [idx for idx in engineered_indices if idx < len(importances)]

    if valid_indices:
        eng_importances = np.zeros(len(valid_indices))
        eng_features = []

        for i, idx in enumerate(valid_indices):
            eng_importances[i] = importances[idx]
            eng_features.append(X.columns[idx])

        if len(eng_features) > 0:
            # Ã–nem derecesine gÃ¶re sÄ±rala
            eng_sorted_idx = np.argsort(eng_importances)[::-1]
            # Dizin sÄ±nÄ±rlarÄ±nÄ± kontrol et
            max_features = min(10, len(eng_features))
            top_eng_features = [eng_features[i] for i in eng_sorted_idx[:max_features]]
            top_eng_importances = eng_importances[eng_sorted_idx[:max_features]]

        plt.figure(figsize=(12, 6))
        sns.barplot(x=top_eng_importances, y=top_eng_features, palette="rocket")
        plt.title("En Ã–nemli 10 MÃ¼hendislik YapÄ±lmÄ±ÅŸ Ã–zellik")
        plt.xlabel("Ã–nem Derecesi")
        plt.tight_layout()
        plt.show()
        plt.close()



#######################################################################################################################
# 5. BULGULAR VE Ä°Å Ã–NERÄ°LERÄ°
#######################################################################################################################
print("\n--- BULGULAR VE Ä°Å Ã–NERÄ°LERÄ° ---\n")

# Ã–nemli Ã¶zellikleri al
important_features = []
if 'features' in locals() and len(features) > 0:
    max_features = min(5, len(features))
    important_features = list(features[:max_features])
else:
    important_features = list(X.columns[:5])
    # Ä°lk 5 Ã¶zelliÄŸi al
    # Ã–nemli Ã¶zellikleri yazdÄ±r
print("Ã–nemli Ã–zellikler:")
for feature in important_features:
    print(feature)

# Bulgular
print("\nBulgular:")
print("""
1. SÄ±nÄ±f DengesizliÄŸi ve Modelleme ZorluÄŸu
   - DolandÄ±rÄ±cÄ±lÄ±k iÅŸlemleri tÃ¼m iÅŸlemler arasÄ±nda Ã§ok az (%1â€™in altÄ±nda), bu da modelin fraudâ€™u yakalamasÄ±nÄ± zorlaÅŸtÄ±rÄ±yor.
   - SMOTE gibi dengesiz veri teknikleriyle bu sorun Ã¶nemli Ã¶lÃ§Ã¼de azaltÄ±ldÄ±.

2. Fraud ile YÃ¼ksek Korelasyonlu Ã–zellikler
   - "credit_risk_score", "velocity_24h", "risk_income_ratio", "rfm_score", "is_high_risk" gibi deÄŸiÅŸkenler fraud ile yÃ¼ksek korelasyon gÃ¶steriyor.
   - KÄ±sa adres sÃ¼resi, yÃ¼ksek baÅŸvuru hÄ±zÄ±, alÄ±ÅŸÄ±lmadÄ±k aktiviteler ve dÃ¼ÅŸÃ¼k yaÅŸ fraud iÅŸlemlerde sÄ±kÃ§a gÃ¶zlemleniyor.

3. Eksik ve Anormal DeÄŸerlerin Ã–nemi
   - Eksik ve uÃ§ (anormal) deÄŸerler bazÄ± Ã¶nemli deÄŸiÅŸkenlerde mevcut; bu veriler model iÃ§in hem risk hem fÄ±rsat oluÅŸturuyor.

4. Segmentasyon ile Riskli GruplarÄ±n Ortaya Ã‡Ä±kmasÄ±
   - KÃ¼melenmiÅŸ mÃ¼ÅŸteri segmentlerinde (Ã¶r. yÃ¼ksek riskli clusterâ€™larda) fraud oranÄ± anlamlÄ± derecede yÃ¼ksek.

5. Ã–zellik MÃ¼hendisliÄŸinin KatkÄ±sÄ±
   - Zaman, hÄ±z, oran, anomali ve segmentasyon tabanlÄ± tÃ¼retilmiÅŸ deÄŸiÅŸkenler model baÅŸarÄ±sÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rdÄ±.
""")

# Ä°ÅŸ Ã¶nerileri
print("\nÄ°ÅŸ Ã–nerileri:")
print("""
1. Modelin CanlÄ±ya AlÄ±nmasÄ± ve DÃ¼zenli Ä°zlenmesi
   - BaÅŸarÄ±lÄ± bulunan modelin (Ã¶r. LightGBM/Random Forest) canlÄ±ya alÄ±nmasÄ± ve performansÄ±nÄ±n periyodik olarak izlenmesi.

2. YÃ¼ksek Riskli BaÅŸvurular iÃ§in Alarm Sistemi
   - KÄ±sa adres sÃ¼resi, yÃ¼ksek baÅŸvuru hÄ±zÄ±, yÃ¼ksek risk skoru gibi durumlar iÃ§in otomatik alarm ve ek kontrol sÃ¼reÃ§lerinin devreye alÄ±nmasÄ±.

3. Veri Kalitesi ve Eksik Veri KontrolÃ¼
   - Eksik ve anormal deÄŸerlerin otomatik tespiti ve raporlanmasÄ±, baÅŸvuru sÄ±rasÄ±nda kullanÄ±cÄ±ya uyarÄ± verilmesi.

4. KÃ¼melenmiÅŸ Segment BazlÄ± Takip
   - YÃ¼ksek riskli mÃ¼ÅŸteri segmentlerinin (cluster) ayrÄ± izlenmesi ve segment bazlÄ± gÃ¼venlik Ã¶nlemlerinin uygulanmasÄ±.

5. Ã–zellik ve Model GeliÅŸiminin SÃ¼rekliliÄŸi
   - Modelin ve tÃ¼retilen Ã¶zelliklerin dÃ¼zenli olarak gÃ¼ncellenmesi, yeni fraud davranÄ±ÅŸlarÄ±na karÅŸÄ± sistemin adapte edilmesi.
""")


